{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import together\n",
    "from time import sleep\n",
    "import re\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dprint(s, debug):\n",
    "    if debug:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find your API key here\n",
    "# https://api.together.xyz/settings/api-keys\n",
    "YOUR_API_KEY = '61face25ac35778c0786a2541cac586ca614b28859377010113538d84adf3f0b'\n",
    "together.api_key = YOUR_API_KEY\n",
    "\n",
    "def call_together_api(prompt, student_configs, post_processing, model='meta-llama/Llama-2-7b-chat-hf', debug=False):\n",
    "    output = together.Complete.create(\n",
    "    prompt = prompt,\n",
    "    model = model, \n",
    "    **student_configs\n",
    "    )\n",
    "    dprint('*****prompt*****', debug)\n",
    "    dprint(prompt, debug)\n",
    "    dprint('*****result*****', debug)\n",
    "    res = output['output']['choices'][0]['text']\n",
    "    dprint(res, debug)\n",
    "    dprint('*****output*****', debug)\n",
    "    numbers_only = post_processing(res)\n",
    "    dprint(numbers_only, debug)\n",
    "    dprint('=========', debug)\n",
    "    return numbers_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part 1. Zero Shot Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_addition_pairs(lower_bound, upper_bound, rng):\n",
    "    int_a = int(np.ceil(rng.uniform(lower_bound, upper_bound)))\n",
    "    int_b = int(np.ceil(rng.uniform(lower_bound, upper_bound)))\n",
    "    return int_a, int_b\n",
    "\n",
    "def test_range(added_prompt, prompt_configs, rng, n_sample=30, \n",
    "               lower_bound=1, upper_bound=10, fixed_pairs=None, \n",
    "               pre_processing=lambda x:x, post_processing=lambda y:y,\n",
    "               model='meta-llama/Llama-2-7b-chat-hf', debug=False):\n",
    "    int_as = []\n",
    "    int_bs = []\n",
    "    answers = []\n",
    "    model_responses = []\n",
    "    correct = []\n",
    "    prompts = []\n",
    "    iterations = range(n_sample) if fixed_pairs is None else fixed_pairs\n",
    "    for i, v in enumerate(tqdm(iterations)):\n",
    "        if fixed_pairs is None:\n",
    "            int_a, int_b = get_addition_pairs(lower_bound=lower_bound, upper_bound=upper_bound, rng=rng)\n",
    "        else:\n",
    "            int_a, int_b = v\n",
    "        fixed_prompt = f'{int_a}+{int_b}'\n",
    "        fixed_prompt = pre_processing(fixed_prompt)\n",
    "        prefix, suffix = added_prompt\n",
    "        prompt = prefix + fixed_prompt + suffix\n",
    "        model_response = call_together_api(prompt, prompt_configs, post_processing, model=model, debug=debug)\n",
    "        answer = int_a + int_b\n",
    "        int_as.append(int_a)\n",
    "        int_bs.append(int_b)\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "        model_responses.append(model_response)\n",
    "        correct.append((answer == model_response))\n",
    "        sleep(1) # pause to not trigger DDoS defense\n",
    "    df = pd.DataFrame({'int_a': int_as, 'int_b': int_bs, 'prompt': prompts, 'answer': answers, 'response': model_responses, 'correct': correct})\n",
    "    print(df)\n",
    "    mae = mean_absolute_error(df['answer'], df['response'])\n",
    "    acc = df.correct.sum()/len(df)\n",
    "    prompt_length = len(prefix) + len(suffix)\n",
    "    res = acc * 1/prompt_length * (1-mae/(5*10**6))\n",
    "    return {'res': res, 'acc': acc, 'mae': mae, 'prompt_length': prompt_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",  #LLaMa-2-7B\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\", #LLaMa-2-13B\n",
    "    \"meta-llama/Llama-2-70b-hf\" #LLaMa-2-70B\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Zero-shot single-digit addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:13<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   int_a  int_b                             prompt  answer  response  correct\n",
      "0      7      4   Question: What is 7+4?\\nAnswer:       11        11     True\n",
      "1      2      2   Question: What is 2+2?\\nAnswer:        4         4     True\n",
      "2      9     10  Question: What is 9+10?\\nAnswer:       19        19     True\n",
      "3      7      8   Question: What is 7+8?\\nAnswer:       15        15     True\n",
      "4      6     10  Question: What is 6+10?\\nAnswer:       16         6    False\n",
      "5      9      2   Question: What is 9+2?\\nAnswer:       11        11     True\n",
      "6      9      2   Question: What is 9+2?\\nAnswer:       11        11     True\n",
      "7      8      3   Question: What is 8+3?\\nAnswer:       11         8    False\n",
      "8      9      6   Question: What is 9+6?\\nAnswer:       15        15     True\n",
      "9      4      5   Question: What is 4+5?\\nAnswer:        9         9     True\n",
      "{'res': 0.028571421142857146, 'acc': 0.8, 'mae': 1.3, 'prompt_length': 28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "added_prompt = ('Question: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "prompt_config = {'max_tokens': 2,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "\n",
    "# input_string: 'a+b'\n",
    "def your_pre_processing(input_string):\n",
    "    return input_string\n",
    "\n",
    "# output_string: \n",
    "# depending on your prompt, it might look like 'output: number'\n",
    "def your_post_processing(output_string):\n",
    "    # using regular expression to find the first consecutive digits in the returned string\n",
    "    only_digits = re.sub(r\"\\D\", \"\", output_string)\n",
    "    try:\n",
    "        res = int(only_digits)\n",
    "    except:\n",
    "        res = 0\n",
    "    return res\n",
    "\n",
    "model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "print(model)\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1, upper_bound=10, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model=model, debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Zero-shot 7-digit addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:13<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                        prompt    answer  \\\n",
      "0  6732655  3428081  Question: What is 6732655+3428081?\\nAnswer:   10160736   \n",
      "1  1368762  1148749  Question: What is 1368762+1148749?\\nAnswer:    2517511   \n",
      "2  8319432  9214800  Question: What is 8319432+9214800?\\nAnswer:   17534232   \n",
      "3  6459722  7565469  Question: What is 6459722+7565469?\\nAnswer:   14025191   \n",
      "4  5892625  9415651  Question: What is 5892625+9415651?\\nAnswer:   15308276   \n",
      "5  8342682  1024647  Question: What is 8342682+1024647?\\nAnswer:    9367329   \n",
      "6  8716638  1302271  Question: What is 8716638+1302271?\\nAnswer:   10018909   \n",
      "7  7566899  2580901  Question: What is 7566899+2580901?\\nAnswer:   10147800   \n",
      "8  8768610  5873151  Question: What is 8768610+5873151?\\nAnswer:   14641761   \n",
      "9  3697407  4804185  Question: What is 3697407+4804185?\\nAnswer:    8501592   \n",
      "\n",
      "   response  correct  \n",
      "0   6732655    False  \n",
      "1   2517591    False  \n",
      "2   8319432    False  \n",
      "3   6459722    False  \n",
      "4   5892625    False  \n",
      "5   9367109    False  \n",
      "6   9034889    False  \n",
      "7   7566899    False  \n",
      "8  14639761    False  \n",
      "9   8501592     True  \n",
      "{'res': 0.001200627, 'acc': 0.1, 'mae': 3319122.2, 'prompt_length': 28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "prompt_config['max_tokens'] = 8\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model=model, debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1a.** In your opinion, what are some factors that cause language model performance to deteriorate from 1 digit to 7 digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1b**. Play around with the config parameters ('max_tokens','temperature','top_k','top_p','repetition_penalty') in together.ai's [web UI](https://api.together.xyz/playground/language/togethercomputer/llama-2-7b). \n",
    "* What does each parameter represent?\n",
    "* How does increasing each parameter change the generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1c**. Do 7-digit addition with 70B parameter llama model. \n",
    "* How does the performance change?\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                        prompt    answer  \\\n",
      "0  6732655  3428081  Question: What is 6732655+3428081?\\nAnswer:   10160736   \n",
      "1  1368762  1148749  Question: What is 1368762+1148749?\\nAnswer:    2517511   \n",
      "2  8319432  9214800  Question: What is 8319432+9214800?\\nAnswer:   17534232   \n",
      "3  6459722  7565469  Question: What is 6459722+7565469?\\nAnswer:   14025191   \n",
      "4  5892625  9415651  Question: What is 5892625+9415651?\\nAnswer:   15308276   \n",
      "5  8342682  1024647  Question: What is 8342682+1024647?\\nAnswer:    9367329   \n",
      "6  8716638  1302271  Question: What is 8716638+1302271?\\nAnswer:   10018909   \n",
      "7  7566899  2580901  Question: What is 7566899+2580901?\\nAnswer:   10147800   \n",
      "8  8768610  5873151  Question: What is 8768610+5873151?\\nAnswer:   14641761   \n",
      "9  3697407  4804185  Question: What is 3697407+4804185?\\nAnswer:    8501592   \n",
      "\n",
      "   response  correct  \n",
      "0  10160636    False  \n",
      "1   2517511     True  \n",
      "2  17534232     True  \n",
      "3  14025191     True  \n",
      "4  15308276     True  \n",
      "5   9367329     True  \n",
      "6   8716638    False  \n",
      "7  10147800     True  \n",
      "8  14641761     True  \n",
      "9   3697407    False  \n",
      "{'res': 0.021946722, 'acc': 0.7, 'mae': 610655.6, 'prompt_length': 28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-70b-hf', debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1d.** Here we're giving our language model the prior that the sum of two 7-digit numbers must have a maximum of 8 digits. (by setting max_token=8). What if we remove this prior by increasing the max_token to 20? \n",
    "* Does the model still perform well?\n",
    "* What are some reasons why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                        prompt    answer  \\\n",
      "0  6732655  3428081  Question: What is 6732655+3428081?\\nAnswer:   10160736   \n",
      "1  1368762  1148749  Question: What is 1368762+1148749?\\nAnswer:    2517511   \n",
      "2  8319432  9214800  Question: What is 8319432+9214800?\\nAnswer:   17534232   \n",
      "3  6459722  7565469  Question: What is 6459722+7565469?\\nAnswer:   14025191   \n",
      "4  5892625  9415651  Question: What is 5892625+9415651?\\nAnswer:   15308276   \n",
      "5  8342682  1024647  Question: What is 8342682+1024647?\\nAnswer:    9367329   \n",
      "6  8716638  1302271  Question: What is 8716638+1302271?\\nAnswer:   10018909   \n",
      "7  7566899  2580901  Question: What is 7566899+2580901?\\nAnswer:   10147800   \n",
      "8  8768610  5873151  Question: What is 8768610+5873151?\\nAnswer:   14641761   \n",
      "9  3697407  4804185  Question: What is 3697407+4804185?\\nAnswer:    8501592   \n",
      "\n",
      "           response  correct  \n",
      "0  6732655342808110    False  \n",
      "1           2517591    False  \n",
      "2          71302322    False  \n",
      "3  6459722756546913    False  \n",
      "4  5892625941565115    False  \n",
      "5           9367109    False  \n",
      "6           9034889    False  \n",
      "7  7566899258090199    False  \n",
      "8          14639761    False  \n",
      "9           8501592     True  \n",
      "{'res': -1903707.375294482, 'acc': 0.1, 'mae': 2665190330412274.5, 'prompt_length': 28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "added_prompt = ('Question: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "prompt_config = {'max_tokens': 20,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "\n",
    "# input_string: 'a+b'\n",
    "def your_pre_processing(input_string):\n",
    "    return input_string\n",
    "\n",
    "def your_post_processing(output_string):\n",
    "    first_line = output_string.splitlines()[0]\n",
    "    only_digits = re.sub(r\"\\D\", \"\", first_line)\n",
    "    try:\n",
    "        res = int(only_digits)\n",
    "    except:\n",
    "        res = 0\n",
    "    return res\n",
    "\n",
    "\n",
    "model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "print(model)\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model=model, debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. In Context Learning\n",
    "\n",
    "We will try to improve the performance of 7-digit addition via in-context learning.\n",
    "For cost-control purposes (you only have $25 free credits), we will use [llama-2-7b](https://api.together.xyz/playground/language/togethercomputer/llama-2-7b). Below is a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                             prompt  \\\n",
      "0  6732655  3428081  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "1  1368762  1148749  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "2  8319432  9214800  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "3  6459722  7565469  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "4  5892625  9415651  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "5  8342682  1024647  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "6  8716638  1302271  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "7  7566899  2580901  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "8  8768610  5873151  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "9  3697407  4804185  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "\n",
      "     answer  response  correct  \n",
      "0  10160736  10154639    False  \n",
      "1   2517511   2517601    False  \n",
      "2  17534232  17533200    False  \n",
      "3  14025191   7324688    False  \n",
      "4  15308276  10307766    False  \n",
      "5   9367329   9367109    False  \n",
      "6  10018909  96250099    False  \n",
      "7  10147800   7817800    False  \n",
      "8  14641761  14640761    False  \n",
      "9   8501592   8502572    False  \n",
      "{'res': -0.0, 'acc': 0.0, 'mae': 10027162.2, 'prompt_length': 63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "added_prompt = ('Question: What is 3+7?\\nAnswer: 10\\n Question: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "prompt_config = {'max_tokens': 8,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-7b-chat-hf', debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2a**.\n",
    "* How does the performance change with the baseline in-context learning prompt? (compare with \"Example: Zero-shot 7-digit addition\" in Q1)\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove the prior on output length and re-evaluate the performance of our baseline one-shot learning prompt. We need to modify our post processing function to extract the answer from the output sequence. In this case, it is the number in the first line that starts with \"Answer: \"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2b**.\n",
    "* How does the performance change when we relax the output length constraint? (compare with Q2a)\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                             prompt  \\\n",
      "0  6732655  3428081  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "1  1368762  1148749  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "2  8319432  9214800  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "3  6459722  7565469  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "4  5892625  9415651  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "5  8342682  1024647  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "6  8716638  1302271  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "7  7566899  2580901  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "8  8768610  5873151  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "9  3697407  4804185  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "\n",
      "     answer  response  correct  \n",
      "0  10160736  10160892    False  \n",
      "1   2517511   2517511     True  \n",
      "2  17534232  17533200    False  \n",
      "3  14025191   7324601    False  \n",
      "4  15308276  10307766    False  \n",
      "5   9367329   9367109    False  \n",
      "6  10018909  10324999    False  \n",
      "7  10147800   9647800    False  \n",
      "8  14641761  14640461    False  \n",
      "9   8501592   8502592    False  \n",
      "{'res': 0.0011901302222222221, 'acc': 0.1, 'mae': 1251089.8, 'prompt_length': 63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "\n",
    "prompt_config['max_tokens'] = 50 # changed from 8, assuming we don't know the output length\n",
    "                \n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-7b-chat-hf', debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2c.** Let's change our one-shot learning example to something more \"in-distribution\". Previously we were using 1-digit addition as an example. Let's change it to 7-digit addition (1234567+1234567=2469134). \n",
    "* Evaluate the performance with max_tokens = 8.\n",
    "* Evaluate the performance with max_tokens = 50.\n",
    "* How does the performance change from 1-digit example to 7-digit example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                             prompt  \\\n",
      "0  1254878  2118550  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "1  7035620  6824705  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "2  6538466  4453098  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "3  9974889  9827518  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "4  7169878  6854133  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "5  7196020  4500293  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "6  2215869  7493395  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "7  5728189  3792177  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "8  5372518  9005390  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "9  9406391  4220157  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "\n",
      "     answer  response  correct  \n",
      "0   3373428   3373428     True  \n",
      "1  13860325  13858925    False  \n",
      "2  10991564  10988464    False  \n",
      "3  19802407  19802697    False  \n",
      "4  14024011  13924000    False  \n",
      "5  11696313  11696213    False  \n",
      "6   9709264   9612864    False  \n",
      "7   9520366   9520366     True  \n",
      "8  14377908  14375409    False  \n",
      "9  13626548   5626508    False  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'res': 0.0021433928205128205,\n",
       " 'acc': 0.2,\n",
       " 'mae': 820384.0,\n",
       " 'prompt_length': 78}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "prompt_config['max_tokens'] = 8 \n",
    "added_prompt = ('Question: What is 1234567+123457?\\nAnswer: 2469134\\nQuestion: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-7b-chat-hf', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                             prompt  \\\n",
      "0  6143768  3896825  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "1  6348700  4041201  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "2  4524571  9012469  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "3  3044419  6608684  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "4  1756139  8493797  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "5  8083884  3154325  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "6  8888358  1527113  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "7  4025054  2352516  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "8  5053054  8166918  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "9  3075780  1468192  Question: What is 1234567+123457?\\nAnswer: 246...   \n",
      "\n",
      "     answer  response  correct  \n",
      "0  10040593  10034553    False  \n",
      "1  10389901  10389801    False  \n",
      "2  13537040  54369857    False  \n",
      "3   9653103   9653093    False  \n",
      "4  10249936  10257176    False  \n",
      "5  11238209  11237409    False  \n",
      "6  10415471  10459491    False  \n",
      "7   6377570   6377560    False  \n",
      "8  13219972  13129066    False  \n",
      "9   4543972   4543972     True  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'res': 0.00023123223076923082,\n",
       " 'acc': 0.1,\n",
       " 'mae': 4098194.3,\n",
       " 'prompt_length': 78}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "prompt_config['max_tokens'] = 50 \n",
    "test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-7b-chat-hf', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2d.** Let's look at a specific example with large absolute error. \n",
    "* Run the cell at least 5 times. Does the error change each time? Why?\n",
    "* Can you think of a prompt to reduce the error?\n",
    "* Why do you think it would work?\n",
    "* Does it work in practice? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****prompt*****\n",
      "Question: What is 1234567+1354634 ?\n",
      "Answer: Start from the rightmost digit and remember to carry over when necessary = 2589201\n",
      "Question: What is 9090909+1010101 ?\n",
      "Answer: \n",
      "*****result*****\n",
      " Start from the rightmost digit and remember to carry over when necessary = 10102001\n",
      "Question: What is 2345678-1234567 ?\n",
      "Answer: Subt\n",
      "*****output*****\n",
      "10102001\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                             prompt  \\\n",
      "0  9090909  1010101  Question: What is 1234567+1354634 ?\\nAnswer: S...   \n",
      "\n",
      "     answer  response  correct  \n",
      "0  10101010  10102001    False  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'res': 0.0, 'acc': 0.0, 'mae': 991.0, 'prompt_length': 156}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_prompt = \"\"\"Question: What is 1234567+1354634 ?\n",
    "Answer: Start from the rightmost digit and remember to carry over when necessary = 2589201\n",
    "Question: What is \"\"\", \" ?\\nAnswer: \"\n",
    "test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, fixed_pairs=[(9090909,1010101)], pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-7b-chat-hf', debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Prompt-a-thon (autograder & leaderboard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compete with your classmates to see who's best at teach llama to add 7-digit numbers reliably! Submit your ```submission.py``` to enter the leader board!\n",
    "\n",
    "Note: while you can use prompt.txt for debugging and local testing, for the final autograder submission, please use a string (not a file), because autograder cannot find prompt.txt in the testing environment. Sorry about the inconvenience!\n",
    "\n",
    "What you can change:\n",
    "* your_api_key\n",
    "* your_prompt\n",
    "* your_config\n",
    "* your_pre_processing\n",
    "* your_post_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2590-hw4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
